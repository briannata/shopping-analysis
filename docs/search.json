[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amazon Shopping Analysis",
    "section": "",
    "text": "1 Introduction\nWe are exploring shopping habits in the United States, specifically purchases on Amazon from 2018 to 2023. Amazon is the largest online retailer so analyzing its shopping trends gives us insight into people’s needs and wants.\nWe are interested in exploring how the COVID-19 pandemic influenced online shopping trends, particularly how much people spent and what products were the most popular. This dataset has data from before, during and after the pandemic, allowing us to get a better understanding and comparison about how the pandemic affected people’s shopping habits.\nWe also want to analyze when is the most popular time to conduct online shopping, especially the date and time. We are particularly interested in seeing whether these trends correlate with major holidays, such as Christmas, or major sale events, like Black Friday and Prime Day.\nAs for general trends, we want to investigate what are the most popular products and categories for both the whole United States, and for each state. We also want to explore whether demographics (gender, race, sexual orientation, and age) have any correlation with shopping habits. We are also interested in exploring whether income, substance abuse, and education affects people’s spending habits and has any correlation with the kinds of products they purchase on Amazon. It would also be intriguing to see if personal situations, such as having diabetes or using a wheelchair, also affects an individual’s online shopping.\nIn general, we are excited to dive into US Amazon trends and explore what variables have correlations with a person’s shopping habits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nIdentify one or more data sources (see II. D. above) that you propose to draw on for the project. For each, describe how the data are collected and by whom. Describe the format of the data, the frequency of updates, dimensions, and any other relevant information. Note any issues / problems with the data, either known or that you discover. Explain how you plan to import the data. Carefully document your sources with links to the precise data sources that you used. If that is not possible (for example if your data is not available online, then explain that clearly.) (suggested: 1/2 page)\nThere are two datasets, amazon_purchases.csv and survey.csv, from the Harvard paper, “Open e-commerce 1.0: Five years of crowdsourced U.S. Amazon purchase histories with user demographics.” The first dataset was a longitudinal collection of purchase data from 5027 Amazon.com users recruited from the online research platforms, Prolific and CloudResearch. If the user decided to share their data with the Harvard researchers, the data was included in the amazon_purchases.csv dataset. If the user then chose to answer an additional survey about their demographics and additional questions, they were compensated for their participation in this study. All participants had to be 18 years or older, U.S. resident and English speaker, and have an active Amazon account. The data collectors were researchers Alex Berke, Robert Mahari, Sandy Pentland, Kent Larson, and Dana Calacci, affiliated with Harvard, MIT and Penn State. This was a one time study so the data collection is finalized so there are not any updates.\nThe amazon_purchases dataset is a csv file with 1048576 rows and 7 columns while the survey dataset is 5028 rows and 23 columns. There is a common column of participant id, so we will join the two datasets together to get the full picture of the participant’s amazon purchases and survey responses, which will result in a dataset with a total of 1048576 rows and 30 columns. There is missing data for some of the shipping locations, item names (title) etc. which we will have to decide how to deal with that. However, for one of the survey questions there are blank responses to represent non applicable, so we need to distinguish this difference somehow.\nWe will import the data by downloading the csv that was included in the paper. We also have access to their github in case that is useful.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "",
    "text": "2.1.1 Data sources:\nDataset Source\nTheir github",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\nWe decided to look at the missing values for the datasets separately for this step before joining them for data analysis later on.\nImports\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.1\n\n\nCode\nlibrary(readr)\n\n\nWarning: package 'readr' was built under R version 4.4.1\n\n\nCode\nlibrary(naniar)\n\n\nWarning: package 'naniar' was built under R version 4.4.2\n\n\nLoad the amazon purchases data into a dataframe\n\n\nCode\namazon_purchases &lt;- read_csv(\"data/amazon-purchases.csv\")\n\n\nRows: 1850717 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): Shipping Address State, Title, ASIN/ISBN (Product Code), Category,...\ndbl  (2): Purchase Price Per Unit, Quantity\ndate (1): Order Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\noptions(readr.show_col_types = FALSE)\n\n\nSee how many missing values there are in each column\n\n\nCode\nprint(\"Missing values:\")\n\n\n[1] \"Missing values:\"\n\n\nCode\nmissing_values = colSums(is.na(amazon_purchases))\nprint(missing_values)\n\n\n              Order Date  Purchase Price Per Unit                 Quantity \n                       0                        0                        0 \n  Shipping Address State                    Title ASIN/ISBN (Product Code) \n                   87812                    89740                      973 \n                Category        Survey ResponseID \n                   89458                        0 \n\n\nGraph the missing values using the naniar package’s vis_miss function\n\n\nCode\nvis_miss(amazon_purchases, warn_large_data = FALSE)  +\n  labs(\n    title = \"Missing Data Visualization for Amazon Purchase Columns\",\n    x = \"Columns\",\n    y = \"Observations\"\n  )\n\n\n\n\n\n\n\n\n\nWe chose this graph to show the patterns in the missing values in each of the columns. We noticed that observations that had one column value missing did not necessarily have missing values in other columns. There didn’t seem to be a lot of overlap, and the missing values seemed to be random and have no pattern. Shipping address state, title, and category had about 5% of their data values missing, while ASIN/ISBN had less than 0.1% missing values.\nGraph the missing values as a simple bar graph to compare which columns have the most missing values\n\n\nCode\nmissing = setNames(nm=c('colnames', 'missing'),stack(colSums(is.na(amazon_purchases)))[2:1])\n\nggplot(missing, aes(x=colnames, y=missing)) + geom_bar(stat=\"identity\") +ggtitle(\"Distribution of Missing Values by Column Name for Amazon Purchases\") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab(\"Columns\") + ylab(\"Number of Missing Values\")\n\n\n\n\n\n\n\n\n\nWe noticed that shipping address state, title, and category had approximately the same amount of missing values. ASIN/ISBN had a very small amount of missing values, while the rest of the columns had no missing values.\nLoad in the survey dataset as a dataframe\n\n\nCode\nsurvey &lt;- read_csv(\"data/survey.csv\")\n\n\nPrint out the missing values per column in this dataset\n\n\nCode\nprint(\"Missing values:\")\n\n\n[1] \"Missing values:\"\n\n\nCode\nprint(colSums(is.na(survey)))\n\n\n         Survey ResponseID                Q-demos-age \n                         0                          0 \n          Q-demos-hispanic               Q-demos-race \n                         0                          0 \n         Q-demos-education             Q-demos-income \n                         0                          0 \n            Q-demos-gender       Q-sexual-orientation \n                         0                          0 \n             Q-demos-state       Q-amazon-use-howmany \n                         0                          0 \n      Q-amazon-use-hh-size       Q-amazon-use-how-oft \n                         0                          0 \nQ-substance-use-cigarettes  Q-substance-use-marijuana \n                         0                          0 \n   Q-substance-use-alcohol        Q-personal-diabetes \n                         0                          0 \n     Q-personal-wheelchair             Q-life-changes \n                         0                       3384 \n          Q-sell-YOUR-data       Q-sell-consumer-data \n                         0                          0 \n           Q-small-biz-use               Q-census-use \n                         0                          0 \n        Q-research-society \n                         0 \n\n\nQ-life-changes is the only column that has missing values in this survey dataset. This question is optional, which is why we expected to see this pattern of missing values for only this column.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4  Interactive graph",
    "section": "",
    "text": "Age Education Gender Income Race\n\n\n\n\n\nWe used a Python script to group and parse the data so that we could create our grouped bar chart, showing the top five Amazon product categories for each of the different demographic groups.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactive graph</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "library(readr) purchase = read_csv(“data/amazon-purchases.csv”) survey = read_csv(“data/survey.csv”)\n\n4 joined_df = merge(purchase, survey, by = “Survey ResponseID”)\n\n\n5 write.csv(joined_df, “data/joined-amazon-purchases.csv”)\njoined_df = read_csv(“data/joined-amazon-purchases.csv”)\n\n\n6 Graph 1\nlibrary(dplyr) library(ggplot2) library(maps)\n\n\n7 Calculate the total number of purchases in each state\ntotal_purchases_by_state &lt;- joined_df %&gt;% group_by(Q-demos-state) %&gt;% summarise(total_purchases = n()) %&gt;% mutate(Q-demos-state = tolower(Q-demos-state)) %&gt;% arrange(desc(total_purchases))\n\n\n8 View the resulting dataframe\nprint(total_purchases_by_state)\n\n\n9 Load map data for states\nstates &lt;- map_data(“state”)\n\n\n10 Join the purchase data with map data\nstates_purchases &lt;- states %&gt;% rename(Q-demos-state = region) %&gt;% left_join(total_purchases_by_state, by = “Q-demos-state”)\n\n\n11 Plot the data on a map\nggplot(states_purchases, aes(long, lat, group = group, fill = total_purchases)) + geom_polygon(color = “black”) + scale_fill_viridis_c(option = “magma”, na.value = “gray”, direction = -1) + theme_minimal() + labs(title = “Total Amazon Purchases from 2018 to 2023 by State”, fill = “Total Purchases”)\n\n\n12 Graph 2\nlibrary(RColorBrewer)\n\n\n13 Convert Order Date to Date type\npurchase\\(`Order Date` &lt;- as.Date(purchase\\)Order Date, format=“%Y-%m-%d”)\n\n\n14 Extract Year from Order Date\npurchase\\(Year &lt;- format(purchase\\)Order Date, “%Y”)\n\n\n15 Group by Year and Category to count purchases\ncategory_counts &lt;- purchase %&gt;% group_by(Year, Category) %&gt;% summarise(purchases = n()) %&gt;% ungroup()\n\n\n16 Remove ‘NA’ categories and filter out the year 2024\ncategory_counts &lt;- category_counts %&gt;% filter(Category != “NA” & Year != “2024”)\n\n\n17 Get the top 5 most purchased categories for each year\ntop_categories &lt;- category_counts %&gt;% group_by(Year) %&gt;% top_n(5, purchases) %&gt;% ungroup()\n\n\n18 Order categories within each year by number of purchases (tallest to shortest)\ntop_categories\\(Category &lt;- factor(top_categories\\)Category, levels = top_categories %&gt;% group_by(Year) %&gt;% arrange(Year, desc(purchases)) %&gt;% pull(Category) %&gt;% unique())\n\n\n19 Choose a qualitative color palette\npalette &lt;- brewer.pal(8, “Set2”)\n\n\n20 Create the grouped bar graph\nggplot(top_categories, aes(x = Year, y = purchases, fill = Category)) + geom_bar(stat = “identity”, position = “dodge”) + labs(title = “Top 5 Most Purchased Amazon Categories by Year”, x = “Year”, y = “Number of Purchases”) + scale_fill_manual(values = palette) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n21 Graph 3\n\n\n22 Calculate total spending per user\nuser_spending &lt;- joined_df %&gt;% mutate(Total_Spent = Purchase Price Per Unit * Quantity) %&gt;% group_by(Survey ResponseID) %&gt;% summarise(Total_Spent_Per_User = sum(Total_Spent, na.rm = TRUE))\n\n\n23 Plot a histogram of total spending per user\nggplot(user_spending, aes(x = Total_Spent_Per_User)) + geom_histogram(binwidth = 1000, fill = “blue”, color = “black”, alpha = 0.7) + labs( title = “Distribution of Total Spending from 2018 to 2023 Per Account”, x = “Total Spending (USD)”, y = “Number of Accounts” ) + theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  }
]